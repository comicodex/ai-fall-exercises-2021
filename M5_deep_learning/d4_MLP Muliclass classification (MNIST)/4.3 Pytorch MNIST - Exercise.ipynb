{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Multiclass Clasification: MNIST</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxliary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/view-classify-in-module-helper/30279/6\n",
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "          ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0343,  0.0310,  0.0147,  ..., -0.0205, -0.0028,  0.0300],\n",
      "        [-0.0073,  0.0304, -0.0037,  ...,  0.0336, -0.0212,  0.0339],\n",
      "        [-0.0255, -0.0106,  0.0286,  ..., -0.0022, -0.0212, -0.0131],\n",
      "        ...,\n",
      "        [ 0.0276,  0.0335,  0.0340,  ..., -0.0002,  0.0217, -0.0204],\n",
      "        [-0.0003,  0.0304,  0.0278,  ..., -0.0314,  0.0294, -0.0187],\n",
      "        [ 0.0293, -0.0178, -0.0053,  ...,  0.0218,  0.0092,  0.0143]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0238,  0.0328,  0.0061, -0.0145, -0.0087, -0.0192, -0.0043,  0.0117,\n",
      "         0.0341, -0.0261, -0.0354, -0.0268,  0.0352, -0.0063,  0.0109,  0.0043,\n",
      "        -0.0017,  0.0032, -0.0296, -0.0275, -0.0075,  0.0078, -0.0255, -0.0208,\n",
      "         0.0088, -0.0230,  0.0281,  0.0172, -0.0320, -0.0192,  0.0057, -0.0288,\n",
      "         0.0263, -0.0051,  0.0053,  0.0348,  0.0143,  0.0113,  0.0137, -0.0243,\n",
      "        -0.0246, -0.0108,  0.0260, -0.0052,  0.0050,  0.0251,  0.0105,  0.0222,\n",
      "         0.0208, -0.0240,  0.0342,  0.0177,  0.0328, -0.0120, -0.0162,  0.0146,\n",
      "         0.0329, -0.0007, -0.0030,  0.0115, -0.0102, -0.0267, -0.0076,  0.0289,\n",
      "        -0.0046,  0.0302, -0.0028,  0.0141,  0.0085,  0.0130,  0.0351, -0.0107,\n",
      "        -0.0087, -0.0011,  0.0262, -0.0210,  0.0196,  0.0034, -0.0278, -0.0011,\n",
      "        -0.0325,  0.0188, -0.0228,  0.0279, -0.0298, -0.0355,  0.0349,  0.0090,\n",
      "         0.0180,  0.0287,  0.0214, -0.0145,  0.0221,  0.0141, -0.0264,  0.0152,\n",
      "         0.0323,  0.0353, -0.0011, -0.0352,  0.0210,  0.0344,  0.0091, -0.0283,\n",
      "         0.0254, -0.0041, -0.0063, -0.0176,  0.0208,  0.0057,  0.0311,  0.0231,\n",
      "        -0.0260,  0.0250,  0.0141, -0.0112,  0.0087,  0.0295,  0.0155,  0.0199,\n",
      "        -0.0162,  0.0173,  0.0107, -0.0114, -0.0159, -0.0147,  0.0076, -0.0157],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0102, -0.0009, -0.0060,  ..., -0.0002,  0.0123,  0.0128],\n",
       "        [ 0.0190, -0.0089, -0.0019,  ..., -0.0094, -0.0004,  0.0076],\n",
       "        [ 0.0079, -0.0039,  0.0045,  ..., -0.0021,  0.0033,  0.0120],\n",
       "        ...,\n",
       "        [ 0.0110,  0.0076,  0.0035,  ...,  0.0015,  0.0052, -0.0125],\n",
       "        [-0.0039, -0.0203,  0.0048,  ..., -0.0023,  0.0069,  0.0106],\n",
       "        [ 0.0086,  0.0026,  0.0115,  ...,  0.0112,  0.0002, -0.0085]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1769, -0.1684],\n",
      "        [ 0.2978,  1.3442]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0313, 0.0284],\n",
      "        [0.0887, 1.8069]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x000001C0582FF310>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4888, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0884, -0.0842],\n",
      "        [ 0.1489,  0.6721]])\n",
      "tensor([[-0.0884, -0.0842],\n",
      "        [ 0.1489,  0.6721]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0237,  0.0013,  0.0082,  ...,  0.0236, -0.0110, -0.0352],\n",
      "        [-0.0250,  0.0276, -0.0147,  ...,  0.0356,  0.0031, -0.0014],\n",
      "        [-0.0141, -0.0349, -0.0321,  ...,  0.0346, -0.0087,  0.0153],\n",
      "        ...,\n",
      "        [ 0.0205,  0.0130, -0.0251,  ...,  0.0341,  0.0021, -0.0346],\n",
      "        [ 0.0223,  0.0165,  0.0206,  ...,  0.0113,  0.0045,  0.0054],\n",
      "        [-0.0315, -0.0016,  0.0117,  ..., -0.0101, -0.0343,  0.0079]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-6.1660e-04, -6.1660e-04, -6.1660e-04,  ..., -6.1660e-04,\n",
      "         -6.1660e-04, -6.1660e-04],\n",
      "        [-4.3369e-05, -4.3369e-05, -4.3369e-05,  ..., -4.3369e-05,\n",
      "         -4.3369e-05, -4.3369e-05],\n",
      "        [-2.3429e-04, -2.3429e-04, -2.3429e-04,  ..., -2.3429e-04,\n",
      "         -2.3429e-04, -2.3429e-04],\n",
      "        ...,\n",
      "        [-9.4309e-04, -9.4309e-04, -9.4309e-04,  ..., -9.4309e-04,\n",
      "         -9.4309e-04, -9.4309e-04],\n",
      "        [-4.0337e-03, -4.0337e-03, -4.0337e-03,  ..., -4.0337e-03,\n",
      "         -4.0337e-03, -4.0337e-03],\n",
      "        [-1.7525e-03, -1.7525e-03, -1.7525e-03,  ..., -1.7525e-03,\n",
      "         -1.7525e-03, -1.7525e-03]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0236,  0.0013,  0.0082,  ...,  0.0236, -0.0110, -0.0352],\n",
      "        [-0.0250,  0.0276, -0.0147,  ...,  0.0356,  0.0031, -0.0014],\n",
      "        [-0.0141, -0.0349, -0.0320,  ...,  0.0346, -0.0087,  0.0153],\n",
      "        ...,\n",
      "        [ 0.0205,  0.0130, -0.0251,  ...,  0.0341,  0.0021, -0.0346],\n",
      "        [ 0.0223,  0.0165,  0.0206,  ...,  0.0113,  0.0045,  0.0055],\n",
      "        [-0.0315, -0.0016,  0.0118,  ..., -0.0101, -0.0343,  0.0079]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0578\n",
      "\tIteration: 40\t Loss: 2.2893\n",
      "\tIteration: 80\t Loss: 2.2643\n",
      "\tIteration: 120\t Loss: 2.2550\n",
      "\tIteration: 160\t Loss: 2.2251\n",
      "\tIteration: 200\t Loss: 2.2054\n",
      "\tIteration: 240\t Loss: 2.1897\n",
      "\tIteration: 280\t Loss: 2.1633\n",
      "\tIteration: 320\t Loss: 2.1354\n",
      "\tIteration: 360\t Loss: 2.1063\n",
      "\tIteration: 400\t Loss: 2.0741\n",
      "\tIteration: 440\t Loss: 2.0328\n",
      "\tIteration: 480\t Loss: 1.9941\n",
      "\tIteration: 520\t Loss: 1.9459\n",
      "\tIteration: 560\t Loss: 1.9019\n",
      "\tIteration: 600\t Loss: 1.8439\n",
      "\tIteration: 640\t Loss: 1.7846\n",
      "\tIteration: 680\t Loss: 1.7238\n",
      "\tIteration: 720\t Loss: 1.6848\n",
      "\tIteration: 760\t Loss: 1.6018\n",
      "\tIteration: 800\t Loss: 1.5425\n",
      "\tIteration: 840\t Loss: 1.4575\n",
      "\tIteration: 880\t Loss: 1.3844\n",
      "\tIteration: 920\t Loss: 1.3500\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0342\n",
      "\tIteration: 40\t Loss: 1.2298\n",
      "\tIteration: 80\t Loss: 1.1877\n",
      "\tIteration: 120\t Loss: 1.1272\n",
      "\tIteration: 160\t Loss: 1.0805\n",
      "\tIteration: 200\t Loss: 1.0387\n",
      "\tIteration: 240\t Loss: 0.9966\n",
      "\tIteration: 280\t Loss: 0.9466\n",
      "\tIteration: 320\t Loss: 0.9197\n",
      "\tIteration: 360\t Loss: 0.8996\n",
      "\tIteration: 400\t Loss: 0.8664\n",
      "\tIteration: 440\t Loss: 0.8254\n",
      "\tIteration: 480\t Loss: 0.8081\n",
      "\tIteration: 520\t Loss: 0.7827\n",
      "\tIteration: 560\t Loss: 0.7342\n",
      "\tIteration: 600\t Loss: 0.7431\n",
      "\tIteration: 640\t Loss: 0.7154\n",
      "\tIteration: 680\t Loss: 0.6971\n",
      "\tIteration: 720\t Loss: 0.6914\n",
      "\tIteration: 760\t Loss: 0.6790\n",
      "\tIteration: 800\t Loss: 0.6108\n",
      "\tIteration: 840\t Loss: 0.6178\n",
      "\tIteration: 880\t Loss: 0.6164\n",
      "\tIteration: 920\t Loss: 0.6028\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0145\n",
      "\tIteration: 40\t Loss: 0.5886\n",
      "\tIteration: 80\t Loss: 0.5765\n",
      "\tIteration: 120\t Loss: 0.5524\n",
      "\tIteration: 160\t Loss: 0.5680\n",
      "\tIteration: 200\t Loss: 0.5632\n",
      "\tIteration: 240\t Loss: 0.5586\n",
      "\tIteration: 280\t Loss: 0.5532\n",
      "\tIteration: 320\t Loss: 0.5266\n",
      "\tIteration: 360\t Loss: 0.5289\n",
      "\tIteration: 400\t Loss: 0.5114\n",
      "\tIteration: 440\t Loss: 0.5170\n",
      "\tIteration: 480\t Loss: 0.5050\n",
      "\tIteration: 520\t Loss: 0.5042\n",
      "\tIteration: 560\t Loss: 0.4701\n",
      "\tIteration: 600\t Loss: 0.4644\n",
      "\tIteration: 640\t Loss: 0.4765\n",
      "\tIteration: 680\t Loss: 0.4940\n",
      "\tIteration: 720\t Loss: 0.4686\n",
      "\tIteration: 760\t Loss: 0.4522\n",
      "\tIteration: 800\t Loss: 0.4746\n",
      "\tIteration: 840\t Loss: 0.4675\n",
      "\tIteration: 880\t Loss: 0.4533\n",
      "\tIteration: 920\t Loss: 0.4412\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">MNIST Clasification: Exercise</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 1:</h3>\n",
    "  <p>Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.</p>\n",
    "  <p>Build a network to classify the MNIST images with 3 hidden layers. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [400, 200, 100]\n",
    "output_size   = 10\n",
    "\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[2], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 2:</h3>\n",
    "  <p>Train your network implementing the Pytorch training loop and <strong style=\"color:#01ff84\">after each epoch, use the model for predicting the test (validation) MNIST data.</strong></p>\n",
    "  <p>Note: If your model does not fit with the final softmax layer, you can remove this layer.</p>\n",
    "  <p>Hint: <a href=\"https://discuss.pytorch.org/t/training-loop-checking-validation-accuracy/78399\">Training loop checking validation accuracy\n",
    "</a></p>\n",
    "  <p>Research about <code>model.train()</code>, <code>model.eval()</code> and <code>with torch.no_grad()</code> in Pytorch.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0576\n",
      "3.125\n",
      "9.375\n",
      "9.895833333333334\n",
      "11.71875\n",
      "11.25\n",
      "11.979166666666666\n",
      "12.723214285714286\n",
      "12.890625\n",
      "13.541666666666666\n",
      "12.8125\n",
      "13.210227272727273\n",
      "13.802083333333334\n",
      "13.701923076923077\n",
      "13.392857142857142\n",
      "13.541666666666666\n",
      "13.4765625\n",
      "13.786764705882353\n",
      "14.0625\n",
      "13.81578947368421\n",
      "13.75\n",
      "13.69047619047619\n",
      "13.991477272727273\n",
      "13.994565217391305\n",
      "14.0625\n",
      "13.875\n",
      "14.002403846153847\n",
      "14.00462962962963\n",
      "14.118303571428571\n",
      "14.116379310344827\n",
      "14.010416666666666\n",
      "13.860887096774194\n",
      "13.671875\n",
      "13.731060606060606\n",
      "13.648897058823529\n",
      "13.4375\n",
      "13.237847222222221\n",
      "13.133445945945946\n",
      "13.240131578947368\n",
      "13.14102564102564\n",
      "13.0078125\n",
      "\tIteration: 40\t Loss: 2.3023\n",
      "12.957317073170731\n",
      "13.169642857142858\n",
      "13.117732558139535\n",
      "13.245738636363637\n",
      "13.194444444444445\n",
      "13.247282608695652\n",
      "13.264627659574469\n",
      "13.216145833333334\n",
      "13.26530612244898\n",
      "13.25\n",
      "13.327205882352942\n",
      "13.28125\n",
      "13.237028301886792\n",
      "13.049768518518519\n",
      "13.125\n",
      "13.030133928571429\n",
      "12.993421052631579\n",
      "12.877155172413794\n",
      "13.135593220338983\n",
      "13.098958333333334\n",
      "13.063524590163935\n",
      "13.004032258064516\n",
      "13.020833333333334\n",
      "12.98828125\n",
      "12.95673076923077\n",
      "13.020833333333334\n",
      "12.989738805970148\n",
      "13.074448529411764\n",
      "13.111413043478262\n",
      "13.147321428571429\n",
      "13.248239436619718\n",
      "13.216145833333334\n",
      "13.184931506849315\n",
      "13.133445945945946\n",
      "13.125\n",
      "13.096217105263158\n",
      "13.027597402597403\n",
      "13.000801282051283\n",
      "13.014240506329115\n",
      "12.98828125\n",
      "\tIteration: 80\t Loss: 2.3022\n",
      "13.020833333333334\n",
      "13.090701219512194\n",
      "12.989457831325302\n",
      "12.927827380952381\n",
      "13.014705882352942\n",
      "13.045058139534884\n",
      "13.00287356321839\n",
      "13.014914772727273\n",
      "13.044241573033707\n",
      "12.96875\n",
      "12.99793956043956\n",
      "13.111413043478262\n",
      "13.054435483870968\n",
      "13.081781914893616\n",
      "13.075657894736842\n",
      "13.0859375\n",
      "13.112113402061855\n",
      "13.058035714285714\n",
      "13.05239898989899\n",
      "13.09375\n",
      "13.072400990099009\n",
      "13.097426470588236\n",
      "13.152305825242719\n",
      "13.17608173076923\n",
      "13.154761904761905\n",
      "13.222287735849056\n",
      "13.230140186915888\n",
      "13.266782407407407\n",
      "13.274082568807339\n",
      "13.252840909090908\n",
      "13.260135135135135\n",
      "13.253348214285714\n",
      "13.205199115044248\n",
      "13.212719298245615\n",
      "13.233695652173912\n",
      "13.28125\n",
      "13.287927350427351\n",
      "13.320974576271187\n",
      "13.30094537815126\n",
      "13.372395833333334\n",
      "\tIteration: 120\t Loss: 2.3020\n",
      "13.352272727272727\n",
      "13.332479508196721\n",
      "13.376524390243903\n",
      "13.394657258064516\n",
      "13.3625\n",
      "13.355654761904763\n",
      "13.336614173228346\n",
      "13.330078125\n",
      "13.372093023255815\n",
      "13.353365384615385\n",
      "13.311068702290076\n",
      "13.293087121212121\n",
      "13.322368421052632\n",
      "13.33955223880597\n",
      "13.356481481481481\n",
      "13.396139705882353\n",
      "13.480839416058394\n",
      "13.47373188405797\n",
      "13.466726618705035\n",
      "13.392857142857142\n",
      "13.375443262411348\n",
      "13.347271126760564\n",
      "13.395979020979022\n",
      "13.346354166666666\n",
      "13.372844827586206\n",
      "13.366866438356164\n",
      "13.414115646258503\n",
      "13.376266891891891\n",
      "13.380872483221477\n",
      "13.375\n",
      "13.37955298013245\n",
      "13.342927631578947\n",
      "13.368055555555555\n",
      "13.331980519480519\n",
      "13.336693548387096\n",
      "13.361378205128204\n",
      "13.355891719745223\n",
      "13.370253164556962\n",
      "13.35495283018868\n",
      "13.33984375\n",
      "\tIteration: 160\t Loss: 2.3020\n",
      "13.373447204968944\n",
      "13.348765432098766\n",
      "13.314800613496933\n",
      "13.32888719512195\n",
      "13.323863636363637\n",
      "13.356551204819278\n",
      "13.360778443113773\n",
      "13.439360119047619\n",
      "13.433801775147929\n",
      "13.409926470588236\n",
      "13.35891812865497\n",
      "13.335755813953488\n",
      "13.285765895953757\n",
      "13.254310344827585\n",
      "13.214285714285714\n",
      "13.201349431818182\n",
      "13.223870056497175\n",
      "13.193469101123595\n",
      "13.17213687150838\n",
      "13.168402777777779\n",
      "13.1560773480663\n",
      "13.152472527472527\n",
      "13.131830601092895\n",
      "13.128396739130435\n",
      "13.133445945945946\n",
      "13.121639784946236\n",
      "13.109959893048128\n",
      "13.106715425531915\n",
      "13.153108465608465\n",
      "13.133223684210526\n",
      "13.162630890052355\n",
      "13.142903645833334\n",
      "13.163860103626943\n",
      "13.144329896907216\n",
      "13.149038461538462\n",
      "13.121811224489797\n",
      "13.118654822335026\n",
      "13.170770202020202\n",
      "13.167399497487438\n",
      "13.140625\n",
      "\tIteration: 200\t Loss: 2.3022\n",
      "13.137437810945274\n",
      "13.142017326732674\n",
      "13.14655172413793\n",
      "13.135723039215685\n",
      "13.117378048780488\n",
      "13.099211165048544\n",
      "13.08876811594203\n",
      "13.115985576923077\n",
      "13.12799043062201\n",
      "13.110119047619047\n",
      "13.10722748815166\n",
      "13.10436320754717\n",
      "13.108861502347418\n",
      "13.127920560747663\n",
      "13.13953488372093\n",
      "13.136574074074074\n",
      "13.119239631336406\n",
      "13.094896788990825\n",
      "13.120719178082192\n",
      "13.146306818181818\n",
      "13.178733031674208\n",
      "13.189752252252251\n",
      "13.165639013452914\n",
      "13.176618303571429\n",
      "13.194444444444445\n",
      "13.19137168141593\n",
      "13.181442731277533\n",
      "13.164747807017545\n",
      "13.168668122270743\n",
      "13.152173913043478\n",
      "13.14258658008658\n",
      "13.166756465517242\n",
      "13.18401287553648\n",
      "13.201121794871796\n",
      "13.21808510638298\n",
      "13.201800847457626\n",
      "13.198839662447257\n",
      "13.163077731092438\n",
      "13.192991631799163\n",
      "13.190104166666666\n",
      "\tIteration: 240\t Loss: 2.3020\n",
      "13.193724066390041\n",
      "13.210227272727273\n",
      "13.18801440329218\n",
      "13.185194672131148\n",
      "13.169642857142858\n",
      "13.166920731707316\n",
      "13.170546558704453\n",
      "13.167842741935484\n",
      "13.152610441767068\n",
      "13.1625\n",
      "13.172310756972111\n",
      "13.175843253968255\n",
      "13.160820158102768\n",
      "13.13976377952756\n",
      "13.143382352941176\n",
      "13.12255859375\n",
      "13.126215953307392\n",
      "13.111676356589147\n",
      "13.133445945945946\n",
      "13.118990384615385\n",
      "13.122605363984674\n",
      "13.126192748091603\n",
      "13.13569391634981\n",
      "13.151041666666666\n",
      "13.172169811320755\n",
      "13.163768796992482\n",
      "13.178838951310862\n",
      "13.182136194029852\n",
      "13.179600371747211\n",
      "13.159722222222221\n",
      "13.151522140221402\n",
      "13.149126838235293\n",
      "13.152472527472527\n",
      "13.155793795620438\n",
      "13.147727272727273\n",
      "13.16802536231884\n",
      "13.171254512635379\n",
      "13.180080935251798\n",
      "13.183243727598565\n",
      "13.175223214285714\n",
      "\tIteration: 280\t Loss: 2.3023\n",
      "13.18394128113879\n",
      "13.175975177304965\n",
      "13.162544169611307\n",
      "13.138204225352112\n",
      "13.152412280701755\n",
      "13.155594405594405\n",
      "13.131533101045296\n",
      "13.123914930555555\n",
      "13.132569204152249\n",
      "13.157327586206897\n",
      "13.165807560137457\n",
      "13.174229452054794\n",
      "13.18259385665529\n",
      "13.169642857142858\n",
      "13.172669491525424\n",
      "13.19679054054054\n",
      "13.231271043771043\n",
      "13.223573825503356\n",
      "13.226379598662207\n",
      "13.21875\n",
      "13.205980066445182\n",
      "13.198468543046358\n",
      "13.201320132013201\n",
      "13.214432565789474\n",
      "13.217213114754099\n",
      "13.230187908496733\n",
      "13.253257328990228\n",
      "13.235592532467532\n",
      "13.233211974110032\n",
      "13.220766129032258\n",
      "13.233520900321544\n",
      "13.251201923076923\n",
      "13.23382587859425\n",
      "13.21656050955414\n",
      "13.204365079365079\n",
      "13.221914556962025\n",
      "13.224566246056783\n",
      "13.227201257861635\n",
      "13.195532915360502\n",
      "13.1982421875\n",
      "\tIteration: 320\t Loss: 2.3022\n",
      "13.21553738317757\n",
      "13.208462732919255\n",
      "13.191756965944272\n",
      "13.20408950617284\n",
      "13.216346153846153\n",
      "13.228527607361963\n",
      "13.22152140672783\n",
      "13.219321646341463\n",
      "13.207636778115502\n",
      "13.210227272727273\n",
      "13.241125377643504\n",
      "13.224774096385541\n",
      "13.231981981981981\n",
      "13.229790419161677\n",
      "13.232276119402986\n",
      "13.225446428571429\n",
      "13.227930267062314\n",
      "13.221153846153847\n",
      "13.209808259587021\n",
      "13.207720588235293\n",
      "13.205645161290322\n",
      "13.199013157894736\n",
      "13.196975218658892\n",
      "13.172238372093023\n",
      "13.17481884057971\n",
      "13.190932080924856\n",
      "13.17092939481268\n",
      "13.17798132183908\n",
      "13.189469914040115\n",
      "13.21875\n",
      "13.225605413105413\n",
      "13.219105113636363\n",
      "13.21264164305949\n",
      "13.215042372881356\n",
      "13.217429577464788\n",
      "13.202247191011235\n",
      "13.174019607843137\n",
      "13.16340782122905\n",
      "13.157207520891365\n",
      "13.159722222222221\n",
      "\tIteration: 360\t Loss: 2.3022\n",
      "13.149238227146814\n",
      "13.138812154696133\n",
      "13.137052341597796\n",
      "13.143887362637363\n",
      "13.125\n",
      "13.119023224043715\n",
      "13.104564032697548\n",
      "13.0859375\n",
      "13.075880758807589\n",
      "13.08277027027027\n",
      "13.085411051212938\n",
      "13.09643817204301\n",
      "13.094839142091153\n",
      "13.093248663101605\n",
      "13.108333333333333\n",
      "13.106715425531915\n",
      "13.109250663129973\n",
      "13.099371693121693\n",
      "13.097790237467018\n",
      "13.096217105263158\n",
      "13.098753280839896\n",
      "13.097185863874346\n",
      "13.091546997389035\n",
      "13.09814453125\n",
      "13.129058441558442\n",
      "13.115284974093264\n",
      "13.117732558139535\n",
      "13.104059278350515\n",
      "13.082422879177377\n",
      "13.060897435897436\n",
      "13.059462915601022\n",
      "13.05404974489796\n",
      "13.048664122137405\n",
      "13.043305837563452\n",
      "13.057753164556962\n",
      "13.068181818181818\n",
      "13.066750629722922\n",
      "13.061400753768844\n",
      "13.052161654135338\n",
      "13.03515625\n",
      "\tIteration: 400\t Loss: 2.3025\n",
      "13.033821695760599\n",
      "13.04804104477612\n",
      "13.042803970223325\n",
      "13.037592821782178\n",
      "13.016975308641975\n",
      "13.027247536945813\n",
      "13.037469287469287\n",
      "13.043811274509803\n",
      "13.053942542787286\n",
      "13.052591463414634\n",
      "13.036040145985401\n",
      "13.030946601941748\n",
      "13.033444309927361\n",
      "13.039704106280194\n",
      "13.030873493975903\n",
      "13.022085336538462\n",
      "13.013339328537171\n",
      "12.993421052631579\n",
      "13.010889021479713\n",
      "13.002232142857142\n",
      "12.997327790973872\n",
      "13.007257109004739\n",
      "13.01344562647754\n",
      "12.997494103773585\n",
      "12.985294117647058\n",
      "12.995158450704226\n",
      "12.983021077283372\n",
      "12.96363901869159\n",
      "12.951631701631701\n",
      "12.943313953488373\n",
      "12.942285382830626\n",
      "12.941261574074074\n",
      "12.933025404157044\n",
      "12.935627880184331\n",
      "12.941810344827585\n",
      "12.947964449541285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.943363844393593\n",
      "12.94591894977169\n",
      "12.934225512528474\n",
      "12.933238636363637\n",
      "\tIteration: 440\t Loss: 2.3023\n",
      "12.910997732426305\n",
      "12.906532805429864\n",
      "12.905615124153499\n",
      "12.90822072072072\n",
      "12.914325842696629\n",
      "12.923906950672645\n",
      "12.92994966442953\n",
      "12.932477678571429\n",
      "12.93847438752784\n",
      "12.940972222222221\n",
      "12.96771064301552\n",
      "12.977046460176991\n",
      "12.979442604856512\n",
      "12.974944933920705\n",
      "12.967032967032967\n",
      "12.959155701754385\n",
      "12.958150984682714\n",
      "12.953739082969433\n",
      "12.966367102396514\n",
      "12.972146739130435\n",
      "12.96434381778742\n",
      "12.973484848484848\n",
      "12.975836933045356\n",
      "12.981546336206897\n",
      "12.977150537634408\n",
      "12.989538626609441\n",
      "12.975107066381156\n",
      "12.96741452991453\n",
      "12.963086353944563\n",
      "12.975398936170214\n",
      "12.961119957537155\n",
      "12.970074152542374\n",
      "12.97568710359408\n",
      "12.964794303797468\n",
      "12.976973684210526\n",
      "12.959558823529411\n",
      "12.974973794549266\n",
      "12.964173640167363\n",
      "12.966466597077245\n",
      "12.985026041666666\n",
      "\tIteration: 480\t Loss: 2.3017\n",
      "12.987266112266113\n",
      "12.986255186721992\n",
      "12.972308488612837\n",
      "12.968104338842975\n",
      "12.970360824742269\n",
      "12.972608024691358\n",
      "12.971637577002053\n",
      "12.951460040983607\n",
      "12.947341513292434\n",
      "12.949617346938776\n",
      "12.948701629327902\n",
      "12.950965447154472\n",
      "12.959558823529411\n",
      "12.958628542510121\n",
      "12.967171717171718\n",
      "12.95992943548387\n",
      "12.962147887323944\n",
      "12.970632530120483\n",
      "12.969689378757515\n",
      "12.971875\n",
      "12.974051896207586\n",
      "12.985557768924302\n",
      "12.972166998011929\n",
      "12.96812996031746\n",
      "12.970297029702971\n",
      "12.972455533596838\n",
      "12.974605522682445\n",
      "12.970595472440944\n",
      "12.966601178781925\n",
      "12.962622549019608\n",
      "12.961717221135029\n",
      "12.9638671875\n",
      "12.984283625730994\n",
      "12.974221789883268\n",
      "12.97633495145631\n",
      "12.984496124031008\n",
      "12.989603481624759\n",
      "12.99469111969112\n",
      "12.99373795761079\n",
      "12.992788461538462\n",
      "\tIteration: 520\t Loss: 2.3021\n",
      "12.98884357005758\n",
      "12.984913793103448\n",
      "12.99593690248566\n",
      "13.006917938931299\n",
      "13.00297619047619\n",
      "13.00796102661597\n",
      "13.00699715370019\n",
      "12.991240530303031\n",
      "12.99031190926276\n",
      "12.998231132075471\n",
      "13.000235404896422\n",
      "13.025728383458647\n",
      "13.030605065666041\n",
      "13.047167602996256\n",
      "13.037383177570094\n",
      "13.039295708955224\n",
      "13.044110800744878\n",
      "13.034386617100372\n",
      "13.03339517625232\n",
      "13.02951388888889\n",
      "13.04586414048059\n",
      "13.04485701107011\n",
      "13.046731123388582\n",
      "13.045726102941176\n",
      "13.05045871559633\n",
      "13.04945054945055\n",
      "13.04558957952468\n",
      "13.050296532846716\n",
      "13.046448087431694\n",
      "13.036931818181818\n",
      "13.04162885662432\n",
      "13.043478260869565\n",
      "13.048146473779385\n",
      "13.055618231046932\n",
      "13.057432432432432\n",
      "13.06205035971223\n",
      "13.061041292639139\n",
      "13.051635304659499\n",
      "13.04226296958855\n",
      "13.046875\n",
      "\tIteration: 560\t Loss: 2.3021\n",
      "13.04590017825312\n",
      "13.044928825622776\n",
      "13.049511545293074\n",
      "13.04576684397163\n",
      "13.03650442477876\n",
      "13.0410777385159\n",
      "13.029100529100528\n",
      "13.039172535211268\n",
      "13.038224956063269\n",
      "13.031798245614034\n",
      "13.036339754816112\n",
      "13.032670454545455\n",
      "13.026287085514834\n",
      "13.022648083623693\n",
      "13.03804347826087\n",
      "13.053385416666666\n",
      "13.044302426343155\n",
      "13.043360726643598\n",
      "13.042422279792746\n",
      "13.036099137931034\n",
      "13.03248709122203\n",
      "13.028887457044673\n",
      "13.036020583190394\n",
      "13.037778253424657\n",
      "13.050213675213675\n",
      "13.054607508532424\n",
      "13.048339011925043\n",
      "13.058035714285714\n",
      "13.059741086587437\n",
      "13.058792372881356\n",
      "13.049915397631134\n",
      "13.046347128378379\n",
      "13.058600337268128\n",
      "13.060290404040405\n",
      "13.048844537815127\n",
      "13.040058724832214\n",
      "13.04177135678392\n",
      "13.051316889632107\n",
      "13.04778797996661\n",
      "13.052083333333334\n",
      "\tIteration: 600\t Loss: 2.3020\n",
      "13.061564059900167\n",
      "13.058035714285714\n",
      "13.04933665008292\n",
      "13.048427152317881\n",
      "13.052685950413224\n",
      "13.044038778877887\n",
      "13.045716639209227\n",
      "13.049958881578947\n",
      "13.051621510673234\n",
      "13.045594262295081\n",
      "13.052373158756138\n",
      "13.043811274509803\n",
      "13.053119902120718\n",
      "13.052219055374593\n",
      "13.058943089430894\n",
      "13.050426136363637\n",
      "13.047001620745544\n",
      "13.051173139158577\n",
      "13.055331179321486\n",
      "13.054435483870968\n",
      "13.05354267310789\n",
      "13.062700964630226\n",
      "13.056781701444622\n",
      "13.058393429487179\n",
      "13.065\n",
      "13.076577476038338\n",
      "13.07067384370016\n",
      "13.062300955414013\n",
      "13.063891096979333\n",
      "13.060515873015873\n",
      "13.06210380348653\n",
      "13.068631329113924\n",
      "13.055390995260664\n",
      "13.05451498422713\n",
      "13.051181102362206\n",
      "13.042944182389936\n",
      "13.046997645211931\n",
      "13.05103840125392\n",
      "13.040395148669797\n",
      "13.04443359375\n",
      "\tIteration: 640\t Loss: 2.3022\n",
      "13.041146645865835\n",
      "13.04273753894081\n",
      "13.046753499222396\n",
      "13.050756987577639\n",
      "13.047480620155039\n",
      "13.03937693498452\n",
      "13.033713292117465\n",
      "13.040123456790123\n",
      "13.039291217257318\n",
      "13.038461538461538\n",
      "13.03523425499232\n",
      "13.034413343558283\n",
      "13.038380551301685\n",
      "13.039946483180428\n",
      "13.051049618320612\n",
      "13.054973323170731\n",
      "13.051750380517504\n",
      "13.050911854103344\n",
      "13.057188922610015\n",
      "13.061079545454545\n",
      "13.055503025718608\n",
      "13.045222809667674\n",
      "13.046757164404223\n",
      "13.043580572289157\n",
      "13.040413533834586\n",
      "13.041948198198199\n",
      "13.048163418290855\n",
      "13.04500374251497\n",
      "13.041853512705531\n",
      "13.052705223880597\n",
      "13.047224292101342\n",
      "13.041759672619047\n",
      "13.031667904903417\n",
      "13.033197329376854\n",
      "13.039351851851851\n",
      "13.03855399408284\n",
      "13.030834564254063\n",
      "13.027747050147493\n",
      "13.033873343151694\n",
      "13.026194852941176\n",
      "\tIteration: 680\t Loss: 2.3024\n",
      "13.041483113069017\n",
      "13.049853372434018\n",
      "13.065062225475842\n",
      "13.050529970760234\n",
      "13.049726277372264\n",
      "13.055758017492712\n",
      "13.050400291120814\n",
      "13.042787063953488\n",
      "13.048802612481857\n",
      "13.052536231884059\n",
      "13.047214182344428\n",
      "13.05319725433526\n",
      "13.0501443001443\n",
      "13.035842939481268\n",
      "13.04406474820144\n",
      "13.038793103448276\n",
      "13.026811334289814\n",
      "13.021579512893982\n",
      "13.025304005722461\n",
      "13.020089285714286\n",
      "13.019347360912981\n",
      "13.014155982905983\n",
      "13.008979374110954\n",
      "13.006036931818182\n",
      "13.01418439716312\n",
      "13.011242917847026\n",
      "13.00609971711457\n",
      "12.998764124293785\n",
      "12.995856840620592\n",
      "12.98855633802817\n",
      "12.983473980309423\n",
      "12.984989466292134\n",
      "12.97335203366059\n",
      "12.972689075630251\n",
      "12.98513986013986\n",
      "12.980097765363128\n",
      "12.975069735006974\n",
      "12.97658426183844\n",
      "12.980267732962448\n",
      "12.973090277777779\n",
      "\tIteration: 720\t Loss: 2.3024\n",
      "12.972434119278779\n",
      "12.965287396121884\n",
      "12.97112724757953\n",
      "12.964002071823204\n",
      "12.96551724137931\n",
      "12.971332644628099\n",
      "12.97713204951857\n",
      "12.982915521978022\n",
      "12.988683127572017\n",
      "13.000856164383562\n",
      "12.998033515731874\n",
      "12.997353142076502\n",
      "13.005201227830833\n",
      "12.993869209809265\n",
      "13.008078231292517\n",
      "13.00101902173913\n",
      "12.991858887381275\n",
      "12.999661246612465\n",
      "12.996870771312585\n",
      "13.006756756756756\n",
      "13.008181511470985\n",
      "13.018025606469003\n",
      "13.021534320323015\n",
      "13.018733198924732\n",
      "13.013842281879194\n",
      "13.013153485254692\n",
      "13.020833333333334\n",
      "13.026403743315509\n",
      "13.02152870493992\n",
      "13.025\n",
      "13.02013981358189\n",
      "13.01737034574468\n",
      "13.018758300132802\n",
      "13.018070291777189\n",
      "13.017384105960264\n",
      "13.00843253968254\n",
      "13.013953104359313\n",
      "13.019459102902374\n",
      "13.020833333333334\n",
      "13.016036184210526\n",
      "\tIteration: 760\t Loss: 2.3021\n",
      "13.013304862023652\n",
      "13.014681758530184\n",
      "13.014007208387943\n",
      "13.015379581151832\n",
      "13.016748366013072\n",
      "13.014033942558747\n",
      "13.007252281616688\n",
      "13.014729817708334\n",
      "13.014060468140443\n",
      "13.013392857142858\n",
      "13.016780155642023\n",
      "13.012062823834198\n",
      "13.01948576972833\n",
      "13.012758397932817\n",
      "13.004032258064516\n",
      "12.991301546391753\n",
      "12.99268018018018\n",
      "12.998071979434448\n",
      "13.001444159178433\n",
      "13.008814102564102\n",
      "13.012163892445583\n",
      "13.01150895140665\n",
      "13.010855683269476\n",
      "13.002232142857142\n",
      "13.007563694267516\n",
      "13.008905852417303\n",
      "13.024142312579416\n",
      "13.021494289340101\n",
      "13.018852978453738\n",
      "13.02610759493671\n",
      "13.017541087231352\n",
      "13.022806186868687\n",
      "13.026087641866331\n",
      "13.027392947103275\n",
      "13.018867924528301\n",
      "13.020179020100503\n",
      "13.013644918444166\n",
      "13.018875313283209\n",
      "13.029959324155193\n",
      "13.01953125\n",
      "\tIteration: 800\t Loss: 2.3022\n",
      "13.016931960049938\n",
      "13.008494389027431\n",
      "13.007861145703611\n",
      "13.003342661691542\n",
      "12.996894409937887\n",
      "12.992400744416873\n",
      "12.987918215613384\n",
      "12.985380569306932\n",
      "12.992506180469716\n",
      "12.991898148148149\n",
      "12.989364981504316\n",
      "12.99068657635468\n",
      "12.995848708487085\n",
      "12.989480958230958\n",
      "12.977377300613497\n",
      "12.974877450980392\n",
      "12.972383720930232\n",
      "12.966075794621027\n",
      "12.959783272283273\n",
      "12.957317073170731\n",
      "12.958663215590743\n",
      "12.956204379562044\n",
      "12.963244228432563\n",
      "12.96457827669903\n",
      "12.975378787878787\n",
      "12.974803268765132\n",
      "12.983675937122127\n",
      "12.979317632850242\n",
      "12.980624246079614\n",
      "12.976280120481928\n",
      "12.97570697954272\n",
      "12.97137920673077\n",
      "12.967061824729893\n",
      "12.972122302158274\n",
      "12.975299401197605\n",
      "12.97473086124402\n",
      "12.972296893667862\n",
      "12.971733293556087\n",
      "12.976758045292014\n",
      "12.981770833333334\n",
      "\tIteration: 840\t Loss: 2.3025\n",
      "12.977482164090368\n",
      "12.98062648456057\n",
      "12.974495848161329\n",
      "12.970231042654028\n",
      "12.965976331360947\n",
      "12.970966312056738\n",
      "12.975944510035418\n",
      "12.96985554245283\n",
      "12.967461719670201\n",
      "12.959558823529411\n",
      "12.955346650998825\n",
      "12.956646126760564\n",
      "12.963437866354045\n",
      "12.961065573770492\n",
      "12.960526315789474\n",
      "12.95451226635514\n",
      "12.959451575262543\n",
      "12.958916083916083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.960200814901048\n",
      "12.952398255813954\n",
      "12.951872822299652\n",
      "12.958599187935034\n",
      "12.958067786790266\n",
      "12.95753761574074\n",
      "12.953395953757225\n",
      "12.956480946882218\n",
      "12.95595444059977\n",
      "12.955429147465438\n",
      "12.953107019562715\n",
      "12.950790229885058\n",
      "12.952066590126291\n",
      "12.947964449541285\n",
      "12.958190148911799\n",
      "12.954090389016018\n",
      "12.95357142857143\n",
      "12.958404680365296\n",
      "12.954318700114024\n",
      "12.946682801822323\n",
      "12.939064277588168\n",
      "12.935014204545455\n",
      "\tIteration: 880\t Loss: 2.3026\n",
      "12.938067536889898\n",
      "12.939342403628117\n",
      "12.9335362400906\n",
      "12.936580882352942\n",
      "12.92725988700565\n",
      "12.937358916478555\n",
      "12.942150507328073\n",
      "12.943412162162161\n",
      "12.941155793025871\n",
      "12.930126404494382\n",
      "12.934904601571269\n",
      "12.927410313901346\n",
      "12.926931690929452\n",
      "12.92994966442953\n",
      "12.931215083798882\n",
      "12.941196986607142\n",
      "12.938963210702342\n",
      "12.93847438752784\n",
      "12.936248609566185\n",
      "12.93576388888889\n",
      "12.93007769145394\n",
      "12.926136363636363\n",
      "12.917012735326688\n",
      "12.907909292035399\n",
      "12.907458563535911\n",
      "12.907008830022075\n",
      "12.901391951488423\n",
      "12.897508259911895\n",
      "12.893633113311331\n",
      "12.89320054945055\n",
      "12.896199231613611\n",
      "12.89405153508772\n",
      "12.900465498357065\n",
      "12.90515590809628\n",
      "12.904713114754099\n",
      "12.894036572052402\n",
      "12.890199018538713\n",
      "12.88296568627451\n",
      "12.885949401523394\n",
      "12.887228260869565\n",
      "\tIteration: 920\t Loss: 2.3025\n",
      "12.883414766558088\n",
      "12.886388286334057\n",
      "12.889355362946912\n",
      "12.887242965367966\n",
      "12.878378378378379\n",
      "12.881344492440604\n",
      "12.879247572815533\n",
      "12.87883890086207\n",
      "12.873385360602798\n",
      "12.872983870967742\n",
      "12.87593984962406\n",
      "12.873859978540773\n",
      "12.878483386923902\n",
      "12.886442719486082\n",
      "12.884358288770054\n",
      "12.887286324786325\n",
      "12.898545891141943\n",
      "12.898333333333333\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0576\n",
      "12.89957378795951\n",
      "12.904137839276212\n",
      "12.902046783625732\n",
      "12.896640998406797\n",
      "12.894562334217506\n",
      "12.894144144144144\n",
      "12.888763896241398\n",
      "12.881742464304601\n",
      "12.876386687797147\n",
      "12.87598944591029\n",
      "12.873945703742752\n",
      "12.868615060558188\n",
      "12.871514992109416\n",
      "12.872766684182869\n",
      "12.878937007874017\n",
      "12.876900891452543\n",
      "12.87323205866946\n",
      "12.877747252747254\n",
      "12.877352326189232\n",
      "12.868798955613578\n",
      "12.87330464267084\n",
      "12.866401771756124\n",
      "12.864393545028632\n",
      "12.860764430577223\n",
      "12.862012987012987\n",
      "12.864880643487286\n",
      "12.866122343182996\n",
      "12.860888140859657\n",
      "12.862131401965856\n",
      "12.860142118863049\n",
      "12.853316985028394\n",
      "12.8529525528623\n",
      "12.862248840803709\n",
      "12.861875965002573\n",
      "12.863110539845758\n",
      "12.867552645095017\n",
      "12.868778860954336\n",
      "12.865197334700154\n",
      "12.87442396313364\n",
      "12.867647058823529\n",
      "\tIteration: 40\t Loss: 2.3022\n",
      "12.86886816555953\n",
      "12.866896375701888\n",
      "12.863335033146354\n",
      "12.867740703005603\n",
      "12.876908396946565\n",
      "12.887646161667513\n",
      "12.893600812595226\n",
      "12.896372399797057\n",
      "12.9023061327927\n",
      "12.906645569620252\n",
      "12.90939554881133\n",
      "12.90266548762001\n",
      "12.905413932357396\n",
      "12.909732728189612\n",
      "12.907745591939547\n",
      "12.897898842476094\n",
      "12.89749874308698\n",
      "12.900238573581115\n",
      "12.906108881083794\n",
      "12.907268170426065\n",
      "12.913119679519278\n",
      "12.909579789894947\n",
      "12.907608695652174\n",
      "12.90720169745382\n",
      "12.909912718204488\n",
      "12.91417538614848\n",
      "12.91376306620209\n",
      "12.913351566384883\n",
      "12.919150521609538\n",
      "12.921836228287841\n",
      "12.915220624690134\n",
      "12.919452699356118\n",
      "12.926768926274121\n",
      "12.923257538309441\n",
      "12.925925925925926\n",
      "12.928589047853972\n",
      "12.929706752094628\n",
      "12.930822255046776\n",
      "12.93039842597147\n",
      "12.926904176904177\n",
      "\tIteration: 80\t Loss: 2.3018\n",
      "12.92801914580265\n",
      "12.935262383521334\n",
      "12.93483586477217\n",
      "12.935939794419971\n",
      "12.93398533007335\n",
      "12.94119443087445\n",
      "12.942288921425085\n",
      "12.938810336421257\n",
      "12.942949342425718\n",
      "12.944038929440389\n",
      "12.949684005833738\n",
      "12.949247207382225\n",
      "12.945778748180494\n",
      "12.952920504120213\n",
      "12.950968523002421\n",
      "12.956579583938074\n",
      "12.962179797003383\n",
      "12.958715596330276\n",
      "12.96731789676797\n",
      "12.957831325301205\n",
      "12.966417910447761\n",
      "12.962962962962964\n",
      "12.959514656415186\n",
      "12.962073931829092\n",
      "12.961630695443645\n",
      "12.965680402491614\n",
      "12.963738630923887\n",
      "12.963295074127211\n",
      "12.967331581462016\n",
      "12.965393794749403\n",
      "12.964949928469242\n",
      "12.964506908051453\n",
      "12.974476439790577\n",
      "12.97402520209225\n",
      "12.972090261282661\n",
      "12.97015899383009\n",
      "12.968231389284021\n",
      "12.97222880151587\n",
      "12.97178182678656\n",
      "12.974290780141844\n",
      "\tIteration: 120\t Loss: 2.3019\n",
      "12.975318847425603\n",
      "12.973395469561114\n",
      "12.970002357378595\n",
      "12.966615638247763\n",
      "12.964705882352941\n",
      "12.961330512458861\n",
      "12.963832785345232\n",
      "12.961931018301266\n",
      "12.955637599624941\n",
      "12.950819672131148\n",
      "12.948935423490875\n",
      "12.94997662459093\n",
      "12.953935077066792\n",
      "12.950594960335978\n",
      "12.947261072261073\n",
      "12.945388914764788\n",
      "12.947882736156352\n",
      "12.950371920037192\n",
      "12.948502090106828\n",
      "12.94953596287703\n",
      "12.947670375521557\n",
      "12.94436081519222\n",
      "12.961302637667746\n",
      "12.96087609801202\n",
      "12.963337182448036\n",
      "12.961467466543608\n",
      "12.962482710926695\n",
      "12.960617227084294\n",
      "12.958755177174414\n",
      "12.961206896551724\n",
      "12.959347726228755\n",
      "12.953189536484626\n",
      "12.952773956900504\n",
      "12.953790655061841\n",
      "12.956235697940503\n",
      "12.952960676726109\n",
      "12.951119232526267\n",
      "12.949281150159745\n",
      "12.946021431828546\n",
      "12.93992027334852\n",
      "\tIteration: 160\t Loss: 2.3024\n",
      "12.940942193900774\n",
      "12.934856753069576\n",
      "12.934461608359836\n",
      "12.935485701316386\n",
      "12.94359410430839\n",
      "12.94460806524694\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 300\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                outputs = model.forward(images)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += labels.size(0) \n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print((100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works and predicts well for the validation data\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 3:</h3>\n",
    "  <p>Write the code for adding <strong style=\"color:#01ff84\">Early Stopping with patience = 2</strong> to the training loop from scratch.</p>\n",
    "  <p><strong style=\"color:#01ff84\">Hint:</strong> Monitor the Validation loss every epoch, and if in 2 epochs, the validation loss does not improve, stop the training loop with <code>break</code>.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
